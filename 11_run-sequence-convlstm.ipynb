{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core-sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions in video\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from actions_in_videos.dataset_ucf101 import UCF101, SequenceDataset, DataLoader, ARSequenceDataset\n",
    "from actions_in_videos.models import ResNet50Classifier, ConvLSTM\n",
    "from actions_in_videos.dataset_ucf101 import SequenceBatchShower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__ 1.4.0\n"
     ]
    }
   ],
   "source": [
    "print('torch.__version__', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to save the models if it does not exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = pathlib.Path('model-checkpoints')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model and run options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'channels':3,\n",
    "    'img_dim':224,\n",
    "    'sequence-length':16,\n",
    "    'latent-dim': 512,\n",
    "    'sample-interval': 1,\n",
    "    'dataset-path': '../data/UCF101/',\n",
    "    'batch-size': 32,\n",
    "    'learning-rate': 0.0001,\n",
    "    'num-epochs': 20,\n",
    "    'checkpoint_interval': 2,\n",
    "    'checkpoint-model': False,\n",
    "    'smaller-dataset':False\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_shape = (opt['channels'], opt['img_dim'], opt['img_dim'])\n",
    "\n",
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Training set and its associated Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_collate(batch):\n",
    "    try:\n",
    "        len_batch = len(batch) # original batch length\n",
    "        batch = list(filter (lambda x:x is not None, batch)) # filter out all the Nones\n",
    "        batch = list(filter (lambda x: len(x[0])>0, batch))  # filter out all the empty ones.\n",
    "\n",
    "#         if len_batch > len(batch):\n",
    "#             print('dataset_ucf101:sequence_collate. FILTER!')\n",
    "#             # source all the required samples from the original dataset at random\n",
    "#             diff = len_batch - len(batch)\n",
    "#             for i in range(diff):\n",
    "#                 batch.append(dataset[np.random.randint(0, len(dataset))])\n",
    "        \n",
    "        batch = torch.utils.data.dataloader.default_collate(batch)\n",
    "    except Exception as e:    \n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print(e)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define training set\n",
    "# train_ds = SequenceDataset(\n",
    "#     dataset_path=opt['dataset-path'],\n",
    "#     sequence_length=opt['sequence-length'],\n",
    "#     sample_interval=opt['sample-interval'],\n",
    "#     training=True,\n",
    "#     transform=transforms.Compose(\n",
    "#     [\n",
    "#         transforms.ToPILImage(),\n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         transforms.RandomCrop((224, 224)),\n",
    "#         transforms.Resize((opt['img_dim'], opt['img_dim']), Image.BICUBIC), \n",
    "#         transforms.ToTensor(), \n",
    "#         transforms.Normalize(mean, std),\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size=opt['batch-size'], shuffle=True, num_workers=4)\n",
    "\n",
    "# print(f\"Num of classes: {train_ds.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 9537\n",
      "Num of classes: 101\n"
     ]
    }
   ],
   "source": [
    "train_ds = ARSequenceDataset(dataset_path='../data/UCF101/', \n",
    "                           sequence_length=16, \n",
    "                           training=True, \n",
    "                           transform=None, \n",
    "                           verbose=True, \n",
    "                           smaller_dataset=opt['smaller-dataset'])\n",
    "\n",
    "print(f\"Num of classes: {train_ds.num_classes}\")\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=opt['batch-size'], collate_fn=sequence_collate, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that we are loading the correct stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor \n",
    "# for i, data in enumerate(train_dl):             \n",
    "#     data[0] = data[0].type(dtype)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb = SequenceBatchShower(train_dl, fig_size=10, max_batch_len=4, max_seq_len=6)\n",
    "# print(sb)\n",
    "\n",
    "# sb.showBatch(3, scale=255, permuteToImg=(1,2,3,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = SequenceDataset(\n",
    "#     dataset_path=opt['dataset-path'],\n",
    "#     sequence_length=opt['sequence-length'],\n",
    "#     sample_interval=opt['sample-interval'],\n",
    "#     training=False,\n",
    "#     transform=transforms.Compose(\n",
    "#     [\n",
    "#         transforms.ToPILImage(),\n",
    "#         transforms.Resize((opt['img_dim'], opt['img_dim']), Image.BICUBIC), \n",
    "#         transforms.ToTensor(), \n",
    "#         transforms.Normalize(mean, std),\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "\n",
    "# test_dl = DataLoader(test_ds, batch_size=opt['batch-size'], shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation files: 3783\n",
      "Num of classes: 101\n"
     ]
    }
   ],
   "source": [
    "test_ds = ARSequenceDataset(dataset_path='../data/UCF101/', \n",
    "                           sequence_length=16, \n",
    "                           training=False, \n",
    "                           transform=None, \n",
    "                           verbose=True, \n",
    "                           smaller_dataset=opt['smaller-dataset'])\n",
    "\n",
    "print(f\"Num of classes: {test_ds.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=opt['batch-size'], collate_fn=sequence_collate, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what it contains. Note that in this case, there is no randomness so most likely all frames will be from the same video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb = SequenceBatchShower(test_dl, fig_size=10, max_batch_len=1, max_seq_len=16)\n",
    "# print(sb)\n",
    "\n",
    "# sb.showBatch(2, scale=255, permuteToImg=(1,2,3,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification criterion\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Define network\n",
    "model = ConvLSTM(\n",
    "    num_classes=train_ds.num_classes,\n",
    "    latent_dim=opt['latent-dim'],\n",
    "    lstm_layers=1,\n",
    "    hidden_dim=1024,\n",
    "    bidirectional=True,\n",
    "    attention=True,\n",
    ")\n",
    "model = model.to(device)\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the parameters of the parts we do not want to train and set to true those of the last few layers that we are interested in updating.\n",
    "\n",
    "The dataset is not large enough to warrant training a full ResNet-50 model. \n",
    "\n",
    "To start, we will just fine-tune the output layer and the last BottleNeck block (or residual block). `model.layer4` contains three residual blocks implying model.layer4[2] is the last of these three residual blocks. Fine-tuning only the top layers also reduces the amount of GPU memory, meaning a higher batch size can be used and the model can be trained more quickly with less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "params = []\n",
    "for param in model.lstm.parameters():\n",
    "    param.requires_grad_(True)\n",
    "    params.append(param)\n",
    "for param in model.output_layers.parameters():\n",
    "    param.requires_grad_(True)\n",
    "    params.append(param)\n",
    "for param in model.attention_layer.parameters():\n",
    "    param.requires_grad_(True)\n",
    "    params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params, lr=opt['learning-rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt['checkpoint-model']:\n",
    "    print(f\"Loading checkpoint model: opt['checkpoint-model']\")\n",
    "    model.load_state_dict(torch.load(opt['checkpoint-model']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now everything we need to build the training loop. This is done in the next two functions, the first one is to test the model on the validation set and the second one is the actual training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,epoch):\n",
    "    \"\"\" Evaluate the model on the test set \"\"\"\n",
    "    print(\"\")\n",
    "    model.eval()\n",
    "    test_metrics = {\"loss\": [], \"acc\": []}\n",
    "        \n",
    "    for batch_i, (X, y) in enumerate(tqdm(test_dl, 'Test', leave=False)):\n",
    "        \n",
    "        X = X.permute(0,2,1,3,4)   \n",
    "            \n",
    "        image_sequences = Variable(X.to(device), requires_grad=True).type(dtype) \n",
    "#        image_sequences = Variable(X.to(device), requires_grad=True).float()\n",
    "        labels = Variable(y.to(device), requires_grad=False)\n",
    "            \n",
    "        labels = labels.squeeze()\n",
    "        if len(labels.size()) == 0:\n",
    "            labels = torch.tensor([labels]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Reset LSTM hidden state\n",
    "            model.lstm.reset_hidden_state()\n",
    "            # Get sequence predictions\n",
    "            preds = model(image_sequences)\n",
    "                    \n",
    "        # Compute metrics\n",
    "        acc = 100 * (preds.detach().argmax(1) == labels).cpu().numpy().mean()\n",
    "        loss = criterion(preds, labels).item()\n",
    "        \n",
    "        # Keep track of loss and accuracy\n",
    "        test_metrics[\"loss\"].append(loss)\n",
    "        test_metrics[\"acc\"].append(acc)\n",
    "        \n",
    "        # Log test performance\n",
    "        sys.stdout.write(\n",
    "            \"\\r    Testing | Batch %d/%d | Loss: %f (%f), Acc: %.2f%% (%.2f%%) | \"\n",
    "            % (\n",
    "                batch_i,\n",
    "                len(test_dl),\n",
    "                np.mean(test_metrics[\"loss\"]),\n",
    "                loss,\n",
    "                np.mean(test_metrics[\"acc\"]),\n",
    "                acc,                \n",
    "            )\n",
    "        )    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, train_dl, test_dl):\n",
    "\n",
    "    assert 'checkpoint-model' in opt.keys(), \"Specify path to checkpoint model or set it to False\"\n",
    "    \n",
    "    for epoch in tqdm(range(opt['num-epochs']), desc='Epoch #'):\n",
    "        epoch_metrics = {\"loss\": [], \"acc\": []}\n",
    "        prev_time = time.time()\n",
    "\n",
    "        #print(f\"--- Epoch {epoch} ---\")\n",
    "        for batch_i, (X, y) in enumerate(tqdm(train_dl, 'Train', leave=False)):\n",
    "            model.train()\n",
    "            \n",
    "            #print(X.shape) # [32, 3, 16, 224, 224]\n",
    "            \n",
    "            # dataload outputs size: [batch_size, c, seq_len, h, w]\n",
    "            # we need the input x to be size [batch_size, seq_length, c, h, w ]. \n",
    "            X = X.permute(0,2,1,3,4)   \n",
    "            \n",
    "            #print(X.shape) # [32, 16, 3, 224, 224]\n",
    "            \n",
    "            if X.size(0) == 1:\n",
    "                continue    \n",
    "                \n",
    "                \n",
    "            image_sequences = Variable(X.to(device), requires_grad=True).type(dtype) \n",
    "            labels = Variable(y.to(device), requires_grad=False)\n",
    "            \n",
    "            labels = labels.squeeze()\n",
    "            if len(labels.size()) == 0:\n",
    "                labels = torch.tensor([labels]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Reset LSTM hidden state\n",
    "            model.lstm.reset_hidden_state()\n",
    "\n",
    "            # forward\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Get sequence predictions\n",
    "                preds = model(image_sequences)\n",
    "\n",
    "                # Compute metrics   \n",
    "                loss = criterion(preds, labels)\n",
    "                acc = 100 * (preds.detach().argmax(1) == labels).cpu().numpy().mean()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Keep track of epoch metrics\n",
    "            epoch_metrics[\"loss\"].append(loss.item())\n",
    "            epoch_metrics[\"acc\"].append(acc)\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(train_dl) + batch_i\n",
    "            batches_left = opt['num-epochs'] * len(train_dl) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            sys.stdout.write(\n",
    "                \"\\rEpoch %d/%d | Batch %d/%d | Loss: %f (%f), Acc: %.2f%% (%.2f%%) | ETA: %s\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    opt['num-epochs'],\n",
    "                    batch_i,\n",
    "                    len(train_dl),\n",
    "                    np.mean(epoch_metrics[\"loss\"]),\n",
    "                    loss.item(),\n",
    "                    np.mean(epoch_metrics[\"acc\"]),                    \n",
    "                    acc,\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "#             Empty cache\n",
    "            if torch.cuda.is_available():\n",
    "                 torch.cuda.empty_cache()\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_model(model, epoch)\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        if epoch % opt['checkpoint_interval'] == 0:\n",
    "            os.makedirs('model-checkpoints', exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"model-checkpoints/{model.__class__.__name__}_{epoch}.pth\")\n",
    "            \n",
    "    # Print log\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    sys.stdout.write(\n",
    "        \"\\rEpoch %d/%d | Batch %d/%d | Loss: %f (%f), Acc: %.2f%% (%.2f%%) | ETA: %s\"\n",
    "        % (\n",
    "            epoch+1,\n",
    "            opt['num-epochs'],\n",
    "            batch_i,\n",
    "            len(train_dl),\n",
    "            np.mean(epoch_metrics[\"loss\"]),\n",
    "            loss.item(),            \n",
    "            np.mean(epoch_metrics[\"acc\"]),\n",
    "            acc,\n",
    "            time_left,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # save last model\n",
    "    os.makedirs('model-checkpoints', exist_ok=True)\n",
    "    random_int = int(np.random.uniform(0,1e14))\n",
    "    torch.save(model.state_dict(), f\"model-checkpoints/{model.__class__.__name__}_{epoch}_{random_int}.pth\")\n",
    "    print(\"\")\n",
    "    print(f\"Model saved as: model-checkpoints/{model.__class__.__name__}_{epoch}_{random_int}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt['num-epochs'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcef10aecf7445ba6f181969e90b9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch #', max=3.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=299.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3 | Batch 297/299 | Loss: 2.191058 (1.491686), Acc: 60.56% (71.88%) | ETA: 0:21:47.723522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test', max=119.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Testing | Batch 118/119 | Loss: 3.143807 (3.962840), Acc: 61.49% (14.29%) |  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=299.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Batch 297/299 | Loss: 0.864903 (0.485932), Acc: 85.84% (96.88%) | ETA: 0:12:48.3221668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test', max=119.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Testing | Batch 118/119 | Loss: 1.468541 (2.859514), Acc: 64.72% (14.29%) | \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=299.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Batch 297/299 | Loss: 0.492307 (0.561026), Acc: 92.19% (84.38%) | ETA: 0:00:04.0418409\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test', max=119.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Testing | Batch 118/119 | Loss: 1.309576 (3.297924), Acc: 66.31% (0.00%) |   \n",
      "\n",
      "-------------------------------------------------------\n",
      "Epoch 3/3 | Batch 298/299 | Loss: 0.492307 (0.561026), Acc: 92.19% (84.38%) | ETA: 0:00:04.041840\n",
      "Model saved as: model-checkpoints/ConvLSTM_2_82127395089564.pth\n"
     ]
    }
   ],
   "source": [
    "train_model(model, opt, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as: model-checkpoints/ConvLSTM_last-layers.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f\"model-checkpoints/{model.__class__.__name__}_last-layers.pth\")\n",
    "print(f\"Model saved as: model-checkpoints/{model.__class__.__name__}_last-layers.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfreeze and train again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad_(True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=opt['learning-rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456cd3c274284b59bd4533b7f570ee69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch #', max=4.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=299.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4 | Batch 297/299 | Loss: 0.413146 (0.222269), Acc: 91.52% (93.75%) | ETA: 0:31:06.1319315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test', max=119.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Testing | Batch 118/119 | Loss: 1.563200 (3.480815), Acc: 59.28% (14.29%) |  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=299.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Batch 297/299 | Loss: 0.278821 (0.605692), Acc: 93.93% (90.62%) | ETA: 0:19:57.2000609\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test', max=119.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Testing | Batch 118/119 | Loss: 1.384608 (1.496032), Acc: 63.14% (57.14%) |  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=299.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 | Batch 297/299 | Loss: 0.193620 (0.222469), Acc: 96.12% (90.62%) | ETA: 0:10:44.6329074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test', max=119.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Testing | Batch 118/119 | Loss: 1.449765 (3.072552), Acc: 63.39% (0.00%) |   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=299.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 | Batch 24/299 | Loss: 0.138903 (0.063817), Acc: 96.62% (96.88%) | ETA: 0:09:40.1242838\n",
      "Exception: ../data/UCF101/UCF-101/Basketball/v_Basketball_g16_c04.avi\n",
      "Epoch 3/4 | Batch 297/299 | Loss: 0.169123 (0.156699), Acc: 96.12% (93.75%) | ETA: 0:00:04.0211294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test', max=119.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Testing | Batch 118/119 | Loss: 1.361179 (1.363994), Acc: 64.91% (42.86%) | \n",
      "\n",
      "-------------------------------------------------------\n",
      "Epoch 4/4 | Batch 298/299 | Loss: 0.169123 (0.156699), Acc: 96.12% (93.75%) | ETA: 0:00:04.021129\n",
      "Model saved as: model-checkpoints/ConvLSTM_3_81376913231468.pth\n"
     ]
    }
   ],
   "source": [
    "opt['num-epochs'] = 4\n",
    "train_model(model, opt, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
