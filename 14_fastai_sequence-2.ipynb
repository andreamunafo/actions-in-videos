{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastai-sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Recognition using Fastai\n",
    "\n",
    "> Ref. [3D Resnet for NIfTI images](https://gist.github.com/jcreinhold/78943cdeca1c5fca4a5af5d066bd8a8d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import axes_grid\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.vision.transform import *\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from fastai.metrics import error_rate, accuracy\n",
    "\n",
    "from actions_in_videos.dataset_ucf101 import UCF101\n",
    "from actions_in_videos.avi import AVI\n",
    "from actions_in_videos.models_resnet_3d import resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload packages where content for package development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'fastai version: {fastai.__version__}')\n",
    "print(f'pytorch version: {torch.__version__}')\n",
    "print('python version: {}.{}.{}'.format(sys.version_info.major, sys.version_info.minor, sys.version_info.micro))\n",
    "#print(f'torchvision version: {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom DataSets in Fastai\n",
    "> ref. https://docs.fast.ai/basic_data.html#DataBunch  \n",
    "https://docs.fast.ai/tutorial.itemlist.html  \n",
    "https://blog.usejournal.com/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 16\n",
    "kinetic_mean = np.asarray([0.433, 0.4045, 0.3776],np.float32)\n",
    "kinetic_std = np.asarray([0.1519876, 0.14855877, 0.156976],np.float32)\n",
    "\n",
    "ucf_mean = np.array([0.485, 0.456, 0.406])\n",
    "ucf_std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorsequence2np(seq:Tensor)->np.ndarray:\n",
    "    \"Convert from torch style `sequence` to numpy/matplotlib style.\"\n",
    "    # reshape to have the seq as a single bigger image.\n",
    "    seq = seq.reshape(1,3,seq.shape[0]*seq.shape[2],-1)     \n",
    "    res = seq.cpu().permute(0,2,3,1).numpy().squeeze()  \n",
    "    return res[...,0] if res.shape[2]==1 else res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2tensor(seq:NPArray, dtype:np.dtype)->TensorImage:\n",
    "    \"\"\"\n",
    "    Convert sequence array to torch style image tensor.\n",
    "\n",
    "    Can be used to convert the output of getVideoSequence into a tensor.\n",
    "    For example:\n",
    "    x = getVideoSequence(fn, num_of_frames=self._seq_len)\n",
    "    f = seq2tensor(x, np.float32)\n",
    "    f.shape\n",
    "    torch.Size([16, 3, 240, 320])\n",
    "    \n",
    "    Returns:\n",
    "    - torch tensor of size [seq_len, channel, height, width]\n",
    "    \n",
    "    \"\"\" \n",
    "    try:\n",
    "        a = np.asarray(seq)\n",
    "        if a.ndim==3 : a = np.expand_dims(a,3)\n",
    "        a = np.transpose(a, (0, 3, 1, 2))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         # set_trace()\n",
    "#         import pickle\n",
    "#         f=open('./dump1','wb+'); pickle.dump(a,f); f.close()\n",
    "        a = np.array([])\n",
    "    return torch.from_numpy(a.astype(dtype, copy=False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVideoSequence(filename, num_of_frames=16):\n",
    "    \"\"\"\n",
    "    Extract a sequence of frames from an avi file.\n",
    "    Returns: list of frames.\n",
    "        for ex.  the output of a sequence of sixteen frames in color would be: seq_len, (channel, height, width)\n",
    "                                                                                    16, (240, 320, 3)\n",
    "    \"\"\"\n",
    "    ### load file from AVI\n",
    "    cap = cv2.VideoCapture(str(filename))\n",
    "    if not cap.isOpened(): \n",
    "        print(f\"could not open {filename}\") \n",
    "        return [] \n",
    "    nFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps     = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_index = np.random.randint(nFrames - num_of_frames)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)    \n",
    "    try:\n",
    "        video = []        \n",
    "        f_count = 0\n",
    "        while cap.isOpened() and f_count < num_of_frames:\n",
    "            frameId = cap.get(cv2.CAP_PROP_POS_FRAMES) # current frame number\n",
    "            ret, frame = cap.read() \n",
    "            video.append(frame)\n",
    "            f_count += 1\n",
    "        cap.release() \n",
    "    except:\n",
    "        print(\"\\nException: \" + filename)\n",
    "        video = [] \n",
    "        cap.release() \n",
    "        set_trace()\n",
    "        \n",
    "    # output is with values [0, 255]\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(ItemBase): \n",
    "    \"\"\" Support applying transforms to sequence data (tensor of images) in `px`.        \n",
    "        As all ItemBase classes, it is not usable until you have put it inside your custom ItemList though.        \n",
    "        \"\"\"\n",
    "    def __init__(self, px:Tensor):\n",
    "        self._obj = px # this is the original data.\n",
    "        self._px  = px # this is the property that gets changed.\n",
    "        self._seq_len = self._px.shape[0]        \n",
    "        \n",
    "    def clone(self):\n",
    "        \"Mimic the behavior of torch.clone for `Sequence` objects.\"\n",
    "        return self.__class__(self._px.clone())\n",
    "     \n",
    "    @property\n",
    "    def shape(self)->Tuple[int,int,int,int]: return self._px.shape\n",
    "    @property\n",
    "    def size(self)->Tuple[int,int,int]: return self.shape[-2:]\n",
    "    @property\n",
    "    def device(self)->torch.device: return self._px.device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._px[0])\n",
    "    \n",
    "    def __repr__(self): return f'{self.__class__.__name__} {tuple(self.shape)}'\n",
    "    def _repr_png_(self):  return self._repr_image_format('png')\n",
    "    \n",
    "    def _repr_image_format(self, format_str):        \n",
    "        rows = int(np.ceil(math.sqrt(self._seq_len)))\n",
    "        axs = subplots(rows, rows)      \n",
    "        for x, ax in zip(self._px, axs.flatten()): Image(x).show(ax=ax)\n",
    "        for ax in axs.flatten()[len(self._px):]: ax.axis('off')\n",
    "        plt.tight_layout()            \n",
    "              \n",
    "    def apply_tfms_per_image(self, tfms:TfmList, do_resolve:bool=True, xtra:Optional[Dict[Callable,dict]]=None,\n",
    "                   size:Optional[Union[int,TensorImageSize]]=None, resize_method:ResizeMethod=None,\n",
    "                   mult:int=None, padding_mode:str='reflection', mode:str='bilinear', remove_out:bool=True)->TensorImage:\n",
    "        \"Apply all `tfms` to each `Sequence Image`, if `do_resolve` picks value for random args.\"           \n",
    "        print('apply_tfms')\n",
    "        px_list = []\n",
    "        for i,x in enumerate(self._px):\n",
    "            x = Image(x)\n",
    "            x = x.apply_tfms(tfms, do_resolve, xtra, size, resize_method, mult, padding_mode, mode, remove_out)\n",
    "            px_list.append(x.data.numpy())        \n",
    "        res = np.asarray(px_list, np.float32)\n",
    "        self._px = torch.from_numpy(res.astype(np.float32, copy=False) )\n",
    "        return self\n",
    "    \n",
    "    def apply_tfms(self, tfms:TfmList, do_resolve:bool=True, xtra:Optional[Dict[Callable,dict]]=None,\n",
    "                   size:Optional[Union[int,TensorImageSize]]=None, resize_method:ResizeMethod=None,\n",
    "                   mult:int=None, padding_mode:str='reflection', mode:str='bilinear', remove_out:bool=True)->TensorImage:\n",
    "        \"Apply all `tfms` to a `Sequence`\"    \n",
    "        #print('len of tfms:',len(tfms))\n",
    "        seq = np.asarray(self._px, np.float32)        \n",
    "        if len(tfms) > 1:\n",
    "            # training\n",
    "            seq = self.transform_sequence(seq, training=True, stats=(kinetic_mean, kinetic_std), size=(224, 224), p_flip=0.5, p_crop=0.5, crop_pc=0.7)\n",
    "        else:\n",
    "            seq = self.transform_sequence(seq, training=False, stats=(kinetic_mean, kinetic_std), size=(224, 224), p_flip=0.5, p_crop=0.5, crop_pc=0.7)\n",
    "        #set_trace()\n",
    "        self._px = seq #torch.from_numpy(seq.astype(np.float32, copy=False) )\n",
    "        return self\n",
    "\n",
    " \n",
    "    def transform_sequence(self, clip, training, stats, size=(224, 224), p_flip=0.5, p_crop=0.5, crop_pc=0.7, brightness=30, verbose=False):     \n",
    "        clip = clip.transpose(0,2,3,1) # seq_len, h, w, c\n",
    "        curr_h, curr_w, channels = clip[0].shape  # (240, 320, 3) -> curr_w = 320, curr_h = 240\n",
    "        height, width = size\n",
    "\n",
    "        mean, std = stats\n",
    "\n",
    "        try:            \n",
    "            if training:\n",
    "                ## RANDOM CROP - crop 70-100% of original size\n",
    "                ## don't maintain aspect ratio            \n",
    "                resize_factor_w = (1-crop_pc)*np.random.rand()+crop_pc\n",
    "                resize_factor_h = (1-crop_pc)*np.random.rand()+crop_pc\n",
    "                w1 = int(curr_w*resize_factor_w)\n",
    "                h1 = int(curr_h*resize_factor_h)\n",
    "                w = np.random.randint(curr_w-w1)\n",
    "                h = np.random.randint(curr_h-h1)\n",
    "                coin_crop = np.random.uniform()\n",
    "\n",
    "                coin_flip = np.random.uniform()\n",
    "\n",
    "                ## Brightness +/- 15\n",
    "                random_add = np.random.randint(brightness+1) - brightness/2.0\n",
    "                random_add /=255\n",
    "\n",
    "                data = []\n",
    "                for i, frame in enumerate(clip):\n",
    "                    if verbose: print(f'frame n. {i}')\n",
    "                    if coin_crop < p_crop:\n",
    "                        if verbose: print(\"Add random crop\")\n",
    "                        frame = frame[h:(h+h1),w:(w+w1),:]\n",
    "                    if coin_flip < p_flip:\n",
    "                        if verbose: print(\"Add random flip\")                    \n",
    "                        frame = cv2.flip(frame,1)\n",
    "\n",
    "                    if verbose: print(f'Resizing to ({width,height})')\n",
    "                    frame = cv2.resize(frame,(width,height))\n",
    "                    frame = frame.astype(np.float32)\n",
    "\n",
    "                    if verbose: print(f\"Add random brightness value {random_add}\")                    \n",
    "                    frame += random_add\n",
    "                    frame[frame>1] = 1.0\n",
    "                    frame[frame<0] = 0.0\n",
    "\n",
    "                    # Normalise\n",
    "                    if verbose: \n",
    "                        print(f\"Normalise with mean {mean} and std {std}\")                                \n",
    "                        print(f\"   original: np.mean(frame): {np.mean(frame,axis=(0, 1))}\")\n",
    "                        print(f\"   original: np.std(frame): {np.std(frame,axis=(0, 1))}\")                \n",
    "                    frame = (frame - mean)/std\n",
    "                    if verbose:\n",
    "                        print(f\"   normalised: np.mean(frame): {np.mean(frame,axis=(0, 1))}\")\n",
    "                        print(f\"   normalised: np.std(frame): {np.std(frame,axis=(0, 1))}\")                \n",
    "                    data.append(frame)\n",
    "                data = np.asarray(data)\n",
    "            else:\n",
    "                # don't augment\n",
    "                data = []\n",
    "                for frame in clip:\n",
    "                    frame = cv2.resize(frame,(width,height))\n",
    "                    frame = frame.astype(np.float32)\n",
    "                    frame = (frame - mean)/std\n",
    "                    data.append(frame)\n",
    "                data = np.asarray(data)\n",
    "\n",
    "        except:\n",
    "            print(\"\\nException: transform_sequence failed. \")\n",
    "            data = np.array([])       \n",
    "            set_trace()\n",
    "        # data.shape is: seq_len, h, w, c - this is also the avi.getSequence output                        \n",
    "        # we transpose it to the tensor shape\n",
    "        data = seq2tensor(data, np.float32) #.transpose(0,3,1,2) # seq_len, c, h, w\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def data(self)->TensorImage:\n",
    "        \"Return this sequence pixels as a tensor.\"\n",
    "        return self._px\n",
    "    \n",
    "    def show(self, ax:plt.Axes=None, figsize:tuple=(3,3), title:Optional[str]=None, hide_axis:bool=True,\n",
    "              cmap:str=None, y:Any=None, **kwargs):\n",
    "        \"Show image on `ax` with `title`, using `cmap` if single-channel, overlaid with optional `y`\"\n",
    "        cmap = ifnone(cmap, defaults.cmap)   \n",
    "        if self._seq_len > 1:\n",
    "            image = self._px[0]    \n",
    "        else: image = self._px\n",
    "        ax = show_image(image, ax=ax, hide_axis=hide_axis, cmap=cmap, figsize=figsize)\n",
    "        if y is not None: y.show(ax=ax, **kwargs)\n",
    "        if title is not None: ax.set_title(title)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and use `SequenceDataBunch` to normalise the data. Note that this requires setting the `_bunch` attribute to `SequenceDataBunch` in class `SequenceList(ItemList)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_view_seq(x:Tensor)->Tensor:\n",
    "    \"Make channel the first axis of `x` of a sequence and flatten remaining axes\"\n",
    "    return x.transpose(0,2).contiguous().view(x.shape[2],-1)\n",
    "\n",
    "\n",
    "class SequenceDataBunch(DataBunch):\n",
    "    def batch_stats(self, funcs:Collection[Callable]=None, ds_type:DatasetType=DatasetType.Train)->Tensor:\n",
    "        \"Grab a batch of data and call reduction function `func` per channel\"\n",
    "#        set_trace()\n",
    "        funcs = ifnone(funcs, [torch.mean,torch.std])\n",
    "        x = self.one_batch(ds_type=ds_type, denorm=False)[0].cpu()\n",
    "        return [func(channel_view_seq(x), 1) for func in funcs]\n",
    "\n",
    "    def normalize(self, stats:Collection[Tensor]=None, do_x:bool=True, do_y:bool=False)->None:\n",
    "        \"Add normalize transform using `stats` (defaults to `DataBunch.batch_stats`)\"\n",
    "        if getattr(self,'norm',False): raise Exception('Can not call normalize twice')\n",
    "        if stats is None: self.stats = self.batch_stats()\n",
    "        else:             self.stats = stats\n",
    "        self.norm,self.denorm = normalize_funcs(*self.stats, do_x=do_x, do_y=do_y)\n",
    "        self.add_tfm(self.norm)\n",
    "        print(self.add_tfm)\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceList(ItemList): \n",
    "    \"\"\"\n",
    "       Subclass ImageList to use our own image opening function\n",
    "       This is the main class that allows you to group your inputs or your targets in the data block API. \n",
    "       You can then use any of the splitting or labelling methods before creating a DataBunch.\"\"\"\n",
    "    \n",
    "    # _processor\n",
    "    # _bunch = SequenceDataBunch\n",
    "    _label_cls=None  # If set to one, fastai selects one class that should fit.\n",
    "    \n",
    "    _seq_len=SEQ_LEN\n",
    "    def __init__(self, items, **kwargs):\n",
    "        super().__init__(items, **kwargs)          \n",
    "      \n",
    "    def open(self, fn:PathOrStr, seq_len=16)->Image:\n",
    "        #avi = AVI(fn, verbose=False, img_type=np.float32)\n",
    "        #x = avi.getRandomSequence(seq_len=seq_len, sample_interval=1)\n",
    "        got_video=False\n",
    "        while(got_video==False):\n",
    "            x = getVideoSequence(fn, num_of_frames=self._seq_len)\n",
    "            ## This ndarray image has to be converted to tensor before passing on as fastai Image, we can use pil2tensor                               \n",
    "            x = seq2tensor(x, np.float32)\n",
    "            if len(x.shape) == 4:\n",
    "                got_video = True\n",
    "                break\n",
    "            print(f'Error while reading video: {fn}')        \n",
    "        x.div_(255)  # values are now [0,1]\n",
    "        return Sequence(x)            \n",
    "    \n",
    "    def get(self, i):\n",
    "        \"Defines how to construct an ItemBase from the data in the ItemList.items array\"\n",
    "        fn = super().get(i)\n",
    "        res = self.open(fn, seq_len=self._seq_len)\n",
    "        return res\n",
    "    \n",
    "    def reconstruct(self, t:Tensor): \n",
    "        # this is to remove normalisation if was applied before:        \n",
    "#         mean = np.asarray([0.433, 0.4045, 0.3776],np.float32)\n",
    "#         std = np.asarray([0.1519876, 0.14855877, 0.156976],np.float32)\n",
    "#         seq = np.zeros(t.shape)\n",
    "#         for i, img in enumerate(t):\n",
    "#             for j, (f, m, s) in enumerate(zip(img, mean, std)):  # loop through the channels:  \n",
    "#                 seq[i,j,:,:]=f*s+m # seq = std*t.numpy() + mean        \n",
    "#         return Sequence(torch.from_numpy(seq.astype(np.float32, copy=False) ))\n",
    "        ######################\n",
    "        return Sequence(t.float().clamp(min=0,max=1))\n",
    "            \n",
    "    @classmethod\n",
    "    def from_folder(cls, path, **kwargs):\n",
    "        res = super().from_folder(path, **kwargs)\n",
    "        res.path = path\n",
    "        return res\n",
    "    \n",
    "    def split_by_files(self, names):\n",
    "        return self.split_by_valid_func(lambda o: any(o.name in vn for vn in names))\n",
    "    \n",
    "    def split_by_fname_file(self, fname, path=None, **kwargs):        \n",
    "        \"Split the data by using the names in `fname` for the validation set. `path` will override `self.path`.\"        \n",
    "        path = Path(ifnone(path, self.path))\n",
    "        relative_valid_names = loadtxt_str(path/fname)\n",
    "        valid_names = [str(Path(self.path).joinpath(Path(fn))) for fn in relative_valid_names]        \n",
    "        return self.split_by_files(valid_names)                    \n",
    "    \n",
    "    def show_xys(self, xs, ys, imgsize:int=4, figsize:Optional[Tuple[int,int]]=None, **kwargs):\n",
    "        \"Show the `xs` (inputs) and `ys` (targets) on a figure of `figsize`.\"\n",
    "        rows = int(np.ceil(math.sqrt(len(xs))))\n",
    "        axs = subplots(rows, rows, imgsize=imgsize, figsize=figsize)\n",
    "        for x,y,ax in zip(xs, ys, axs.flatten()):             \n",
    "            x.show(ax=ax, y=y, **kwargs)\n",
    "        for ax in axs.flatten()[len(xs):]: ax.axis('off')\n",
    "        plt.tight_layout()        \n",
    "\n",
    "    def show_xyzs(self, xs, ys, zs, imgsize:int=4, figsize:Optional[Tuple[int,int]]=None, **kwargs):\n",
    "        \"Show `xs` (inputs), `ys` (targets) and `zs` (predictions) on a figure of `figsize`.\"\n",
    "        if self._square_show_res:\n",
    "            title = 'Ground truth\\nPredictions'\n",
    "            rows = int(np.ceil(math.sqrt(len(xs))))\n",
    "            axs = subplots(rows, rows, imgsize=imgsize, figsize=figsize, title=title, weight='bold', size=12)\n",
    "            for x,y,z,ax in zip(xs,ys,zs,axs.flatten()): x.show(ax=ax, title=f'{str(y)}\\n{str(z)}', **kwargs)\n",
    "            for ax in axs.flatten()[len(xs):]: ax.axis('off')\n",
    "        else:\n",
    "            title = 'Ground truth/Predictions'\n",
    "            axs = subplots(len(xs), 2, imgsize=imgsize, figsize=figsize, title=title, weight='bold', size=14)\n",
    "            for i,(x,y,z) in enumerate(zip(xs,ys,zs)):\n",
    "                x.show(ax=axs[i,0], y=y, **kwargs)\n",
    "                x.show(ax=axs[i,1], y=z, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = (SequenceList.from_folder('../data/UCF101/UCF-101/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_collate(batch):\n",
    "    try:\n",
    "        len_batch = len(batch) # original batch length\n",
    "        batch = list(filter (lambda x:x is not None, batch)) # filter out all the Nones\n",
    "        batch = list(filter (lambda x: len(x[0])>0, batch))  # filter out all the empty ones.\n",
    "        \n",
    "        if len_batch > len(batch): \n",
    "            # if there are samples missing just use existing members, doesn't work if you reject every sample in a batch\n",
    "            diff = len_batch - len(batch)\n",
    "            print(f'sequence_collate - diff is {diff}')\n",
    "            \n",
    "        batch = torch.utils.data.dataloader.default_collate(to_data(batch))        \n",
    "    except Exception as e:            \n",
    "        print(e)\n",
    "        set_trace()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_func(o):\n",
    "    if str(o.parents[0]).split('/')[-1] == 'models':\n",
    "        print(o)\n",
    "        print(str(o.parents[0]).split('/')[-1]) \n",
    "    return str(o.parents[0]).split('/')[-1]\n",
    "\n",
    "get_y = lambda o:str(o.parents[0]).split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False, flip_vert=False, max_rotate=0.0, p_affine=0.5, p_lighting=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/UCF101/UCF-101/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (SequenceList.from_folder(data_path) # Where to find the data? -> in path and its subfolders\n",
    "        .split_by_fname_file(fname='testlist01.txt', path='../data/UCF101/annotations/ucfTrainTestlist/') \n",
    "        .label_from_func(get_y_func)         # How to label\n",
    "        .transform(tfms, size=224)           # Data augmentation? -> use tfms with a size of 224\n",
    "        .databunch(bs=64, collate_fn=sequence_collate))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use `SequenceDataBunch` that we can calcuate some statistics. \n",
    "\n",
    "Unnormalised statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.batch_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SequenceDataBunch` also makes available a normalise method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.normalize((kinetic_mean, kinetic_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.batch_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '../data/UCF101/UCF-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = (SequenceList.from_folder(data_path) # Where to find the data? -> in path and its subfolders\n",
    "#         .split_by_fname_file(fname='ucf-5-testlist01.txt', path='../data/UCF101/annotations/ucfTrainTestlist/')             #How to split in train/valid? -> use the folders\n",
    "#         .label_from_func(get_y_func)         # How to label\n",
    "#         .transform(tfms, size=224)           # Data augmentation? -> use tfms with a size of 224\n",
    "#         .databunch(collate_fn=sequence_collate))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_ds.y.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function resnet50\n"
     ]
    }
   ],
   "source": [
    "model =  resnet50(sample_size=224, sample_duration=SEQ_LEN)\n",
    "pretrained = torch.load('./model-pretrained/resnet-50-kinetics.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "keys = [k for k,v in pretrained['state_dict'].items()]\n",
    "pretrained_state_dict = {k[7:]: v.cpu() for k, v in pretrained['state_dict'].items()}\n",
    "model.load_state_dict(pretrained_state_dict)\n",
    "model.fc = nn.Linear(model.fc.weight.shape[1], len(data.train_ds.y.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# for param in model.conv1.parameters():\n",
    "#     param.requires_grad_(True)\n",
    "# for param in model.bn1.parameters():\n",
    "#     param.requires_grad_(True)\n",
    "# for param in model.layer1.parameters():\n",
    "#     param.requires_grad_(True)\n",
    "# for param in model.layer2.parameters():\n",
    "#     param.requires_grad_(True)\n",
    "# for param in model.layer3.parameters():\n",
    "#     param.requires_grad_(True)\n",
    "for param in model.layer4[0].parameters():\n",
    "    param.requires_grad_(True)\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "learner = Learner(data, model, loss_func=loss, metrics=[accuracy], path='./', callback_fns=ShowGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(5,3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y,losses = learn.get_preds(with_loss=True)\n",
    "interp = ClassificationInterpretation(learn, preds, y, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(20,20), dpi=60)\n",
    "interp.plot_top_losses(9, figsize=(20,20), heatmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = rand_zoom(scale=(1.,1.5))\n",
    "\n",
    "rand_resize_crop(size:int, max_scale:float=2.0, ratios:Point=(0.75, 1.33))\n",
    "brightness(x, change:uniform)\n",
    "contrast(x, scale:log_uniform) \n",
    "crop(x, size, row_pct:uniform=0.5, col_pct:uniform=0.5)\n",
    "crop_pad(x, size, padding_mode='reflection', row_pct:uniform=0.5, col_pct:uniform=0.5)\n",
    "perspective_warp(c, magnitude:partial(uniform, size=8)=0, invert=False)\n",
    "\n",
    "\n",
    "RESIZE: data = src.transform(tfms=tfms, size=size, resize_method=ResizeMethod.SQUISH).databunch(bs=bs, num_workers=4).normalize()\n",
    "    squish(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
