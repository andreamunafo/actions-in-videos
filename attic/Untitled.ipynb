{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, dataset_path, split_path, split_number, input_shape, sequence_length, training):\n",
    "        self.training = training\n",
    "        self.label_index = self._extract_label_mapping(split_path)\n",
    "        self.sequences = self._extract_sequence_paths(dataset_path, split_path, split_number, training)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.label_names = sorted(list(set([self._activity_from_path(seq_path) for seq_path in self.sequences])))\n",
    "        self.num_classes = len(self.label_names)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _extract_label_mapping(self, split_path=\"data/ucfTrainTestlist\"):\n",
    "        \"\"\" Extracts a mapping between activity name and softmax index \"\"\"\n",
    "        with open(os.path.join(split_path, \"classInd.txt\")) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        label_mapping = {}\n",
    "        for line in lines:\n",
    "            label, action = line.split()\n",
    "            label_mapping[action] = int(label) - 1\n",
    "        return label_mapping\n",
    "\n",
    "    def _extract_sequence_paths(\n",
    "        self, dataset_path, split_path=\"data/ucfTrainTestlist\", split_number=1, training=True\n",
    "    ):\n",
    "        \"\"\" Extracts paths to sequences given the specified train / test split \"\"\"\n",
    "        assert split_number in [1, 2, 3], \"Split number has to be one of {1, 2, 3}\"\n",
    "        fn = f\"trainlist0{split_number}.txt\" if training else f\"testlist0{split_number}.txt\"\n",
    "        split_path = os.path.join(split_path, fn)\n",
    "        with open(split_path) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        sequence_paths = []\n",
    "        for line in lines:\n",
    "            seq_name = line.split(\".avi\")[0]\n",
    "            sequence_paths += [os.path.join(dataset_path, seq_name)]\n",
    "        return sequence_paths\n",
    "\n",
    "    def _activity_from_path(self, path):\n",
    "        \"\"\" Extracts activity name from filepath \"\"\"\n",
    "        return path.split(\"/\")[-2]\n",
    "\n",
    "    def _frame_number(self, image_path):\n",
    "        \"\"\" Extracts frame number from filepath \"\"\"\n",
    "        return int(image_path.split(\"/\")[-1].split(\".jpg\")[0])\n",
    "\n",
    "    def _pad_to_length(self, sequence):\n",
    "        \"\"\" Pads the sequence to required sequence length \"\"\"\n",
    "        left_pad = sequence[0]\n",
    "        if self.sequence_length is not None:\n",
    "            while len(sequence) < self.sequence_length:\n",
    "                sequence.insert(0, left_pad)\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(len(self))\n",
    "        print(index % len(self))\n",
    "        sequence_path = self.sequences[index % len(self)]\n",
    "        print(self._frame_number(sequence_path))\n",
    "        # Sort frame sequence based on frame number. He needs to do this because he has a list of jpgs..\n",
    "        image_paths = sorted(glob.glob(f\"{sequence_path}/*.jpg\"), key=lambda path: self._frame_number(path))\n",
    "        # Pad frames sequences shorter than `self.sequence_length` to length\n",
    "        image_paths = self._pad_to_length(image_paths)\n",
    "        if self.training:\n",
    "            # Randomly choose sample interval and start frame\n",
    "            sample_interval = np.random.randint(1, len(image_paths) // self.sequence_length + 1)\n",
    "            start_i = np.random.randint(0, len(image_paths) - sample_interval * self.sequence_length + 1)\n",
    "            flip = np.random.random() < 0.5\n",
    "        else:\n",
    "            # Start at first frame and sample uniformly over sequence\n",
    "            start_i = 0\n",
    "            sample_interval = 1 if self.sequence_length is None else len(image_paths) // self.sequence_length\n",
    "            flip = False\n",
    "        # Extract frames as tensors\n",
    "        image_sequence = []\n",
    "        for i in range(start_i, len(image_paths), sample_interval):\n",
    "            if self.sequence_length is None or len(image_sequence) < self.sequence_length:\n",
    "                image_tensor = self.transform(Image.open(image_paths[i]))\n",
    "                if flip:\n",
    "                    image_tensor = torch.flip(image_tensor, (-1,))\n",
    "                image_sequence.append(image_tensor)\n",
    "        image_sequence = torch.stack(image_sequence)\n",
    "        target = self.label_index[self._activity_from_path(sequence_path)]\n",
    "        return image_sequence, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "train_dataset = Dataset(\n",
    "    dataset_path='../data/UCF101',\n",
    "    split_path='../data/UCF101/annotations/ucfTrainTestlist',\n",
    "    split_number=1,\n",
    "    input_shape=(224, 224),\n",
    "    sequence_length=40,\n",
    "    training=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9537\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'v_ApplyEyeMakeup_g08_c01'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a0be1c17c5b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-3ac7fb83b021>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0msequence_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frame_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Sort frame sequence based on frame number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{sequence_path}/*.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frame_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-3ac7fb83b021>\u001b[0m in \u001b[0;36m_frame_number\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_frame_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m\"\"\" Extracts frame number from filepath \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'v_ApplyEyeMakeup_g08_c01'"
     ]
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-28-3ac7fb83b021>\u001b[0m(63)\u001b[0;36m_frame_number\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     61 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m_frame_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     62 \u001b[0;31m        \u001b[0;34m\"\"\" Extracts frame number from filepath \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 63 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     64 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     65 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> image_path.split(\"/\")[-1].split(\".jpg\")\n",
      "['v_ApplyEyeMakeup_g08_c01']\n",
      "ipdb> image_path.split(\"/\")[-1].split(\".jpg\")[0]\n",
      "'v_ApplyEyeMakeup_g08_c01'\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
