{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> This module focuses on preparing the data of the UCF101 dataset to be used with the core functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision  # used to download the model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import resnet50, resnet152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "> Our baseline will be a Convolutional (2D) Classifier.\n",
    "Either based on resnet50 or resnet152."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model from torchvision.\n",
    "\n",
    "Here is the link to the torch zoo of network models. It is worth taking a look at this page: [models](https://pytorch.org/docs/stable/torchvision/models.html).\n",
    "\n",
    "Results are saved in: ~/.cache/torch/checkpoints/resnet50-19c8e357.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNet50Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50Classifier, self).__init__()\n",
    "        \n",
    "        resnet = resnet50(pretrained=True)\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(resnet.fc.in_features, num_classes),\n",
    "#            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, c, h, w = x.shape\n",
    "        x = x.view(batch_size, c, h, w)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.final(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNet152Classifier(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim):\n",
    "        super(ResNet152Classifier, self).__init__()\n",
    "        \n",
    "        resnet = resnet152(pretrained=True)\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(resnet.fc.in_features, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim, momentum=0.01),\n",
    "            nn.Linear(latent_dim, num_classes),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(batch_size * seq_length, -1)\n",
    "        x = self.final(x)\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = resnet152(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(resnet.fc.in_features, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, latent_dim, num_layers, hidden_dim, bidirectional):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, self.hidden_state = self.lstm(x, self.hidden_state)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.latent_attention = nn.Linear(latent_dim, attention_dim)\n",
    "        self.hidden_attention = nn.Linear(hidden_dim, attention_dim)\n",
    "        self.joint_attention = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, latent_repr, hidden_repr):\n",
    "        if hidden_repr is None:\n",
    "            hidden_repr = [\n",
    "                Variable(\n",
    "                    torch.zeros(latent_repr.size(0), 1, self.hidden_attention.in_features), requires_grad=False\n",
    "                ).float()\n",
    "            ]\n",
    "        h_t = hidden_repr[0]\n",
    "        latent_att = self.latent_attention(latent_att)\n",
    "        hidden_att = self.hidden_attention(h_t)\n",
    "        joint_att = self.joint_attention(F.relu(latent_att + hidden_att)).squeeze(-1)\n",
    "        attention_w = F.softmax(joint_att, dim=-1)\n",
    "        return attention_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes, latent_dim=512, lstm_layers=1, hidden_dim=1024, bidirectional=True, attention=True\n",
    "    ):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.lstm = LSTM(latent_dim, lstm_layers, hidden_dim, bidirectional)\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim, momentum=0.01),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            #nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.attention = attention\n",
    "        self.attention_layer = nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        # To verify: print('batch_size, seq_length, c, h, w:', batch_size, seq_length, c, h, w )\n",
    "        # We now take the full batchsize and seq_length and flatten them out.\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        x = self.lstm(x)\n",
    "        if self.attention:\n",
    "            attention_w = F.softmax(self.attention_layer(x).squeeze(-1), dim=-1)\n",
    "            x = torch.sum(attention_w.unsqueeze(-1) * x, dim=1)\n",
    "        else:\n",
    "            x = x[:, -1]\n",
    "        return self.output_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core-baseline.ipynb.\n",
      "Converted 00_core-sequence.ipynb.\n",
      "Converted 01_dataset_ucf101.ipynb.\n",
      "Converted 02_avi.ipynb.\n",
      "Converted 04_data_augmentation.ipynb.\n",
      "Converted 05_models.ipynb.\n",
      "Converted 06_utils.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
